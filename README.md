# ü§ñ FirstAIAgent ‚Äì AI Agent with Local & Hosted LLMs using LangChain

This project demonstrates how to build intelligent agents using [LangChain](https://www.langchain.com/) with two different configurations:
- **Local LLM** via [Ollama](https://ollama.com/)
- **Hosted LLM** via [OpenRouter](https://openrouter.ai/) 

These agents can answer queries using LLM reasoning, math tools, and live search powered by DuckDuckGo.

---

## üìÅ Project Structure

```text
FirstAIAgent/
‚îú‚îÄ‚îÄ AIAgentLocalLLM.py      # Agent using local Ollama model (e.g., Mistral)
‚îú‚îÄ‚îÄ AIAgentHostedLLM.py     # Agent using hosted model via OpenRouter API
‚îú‚îÄ‚îÄ .env                    # Environment variables (API Key and Base URL for hosted LLM)
‚îî‚îÄ‚îÄ .venv/                  # Python virtual environment (optional)
```

---

## ‚öôÔ∏è Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/FirstAIAgent.git
cd FirstAIAgent
```
### 2. (Optional) Create a Virtual Environment)

```bash
python -m venv .venv
source .venv/bin/activate       # macOS/Linux
# .venv\Scripts\activate.bat    # Windows
```
### 3. Install Dependencies

```bash
pip install langchain langchain-community langchain-openai langchain-ollama python-dotenv duckduckgo-search
```
## 4. Create a .env File

```bash
OPENAI_API_KEY=your_api_key_here
OPENAI_API_BASE=https://openrouter.ai/api/v1
```


# ‚ñ∂Ô∏è Run the Agents

## ‚úÖ Local Agent (Ollama)
#### ‚ö†Ô∏è Requires Ollama installed and a local model (e.g., mistral) pulled via ollama pull mistral
```bash
python AIAgentLocalLLM.py
```

## ‚úÖ Hosted Agent (OpenRouter)
#### ‚ö†Ô∏è Ensure .env file is configured correctly
```bash
python AIAgentHostedLLM.py
```